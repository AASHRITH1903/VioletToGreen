import argparse
import os

""""
Script to Evaluate the predictions generated by our tool against the manually annotated set of comment to code links
Pass relevant arguments to evaluate single files or to evaluate the entire dataset.

Run the following command to compare two files:
python3 evaluation.py --method single --code <path of code file> --label <path of annotated file> --prediction <path of predicted file>

Example:
python3 evaluation.py --method single --code ../Dataset/ToyData/CodeFiles/BubbleSort.java --label ../Dataset/ToyData/Annotations/BubbleSort.txt --prediction ../Dataset/ToyData/Predictions/BubbleSort.txt

python3 evaluation.py --method single --code ../Dataset/ToyData/CodeFiles/Empty.java --label ../Dataset/ToyData/Annotations/Empty.txt --prediction ../Dataset/ToyData/Predictions/Empty.txt

To evaluate the entire toy dataset, run the following command:
python3 evaluation.py --method toy_set --verbose

To evaluate the entire real world dataset, run the following command:
python3 evaluation.py --method real_set --verbose
"""

def evaluate_single_file(code_file_name, label_file_name, prediction_file_name, verbose):
    """
    Evaluate the predictions generated by our tool against the manually annotated set of comment to code links
    """
    code_file = open(code_file_name, 'r')
    code_lines = code_file.readlines()
    label_file = open(label_file_name, 'r')
    label_lines = label_file.readlines()
    prediction_file = open(prediction_file_name, 'r')
    prediction_lines = prediction_file.readlines()
    labels = []
    predictions = []
 
    """ print("Code File")
    count = 0
    for line in code_lines:
        count += 1
        print("Line{}: {}".format(count, line)) """

    # print("Label File")
    for line in label_lines:
        parts = line.replace(',',' ').split()
        labels.append({'comment_start_row':int(parts[0]), 
                        'comment_start_char':int(parts[1]),
                        'comment_end_row':int(parts[2]), 
                        'comment_end_char':int(parts[3]), 
                        'code_start_row':int(parts[4]), 
                        'code_start_char':int(parts[5]), 
                        'code_end_row':int(parts[6]), 
                        'code_end_char':int(parts[7])})
    # print(labels)

    # print("Prediction File")
    for line in prediction_lines:
        parts = line.replace(',',' ').split()
        predictions.append({'comment_start_row':int(parts[0]), 
                        'comment_start_char':int(parts[1]),
                        'comment_end_row':int(parts[2]), 
                        'comment_end_char':int(parts[3]), 
                        'code_start_row':int(parts[4]), 
                        'code_start_char':int(parts[5]), 
                        'code_end_row':int(parts[6]), 
                        'code_end_char':int(parts[7])})
    # print(predictions)

    # print('--------------------------------')
    total_accuracy = 0.0
    comment_count = 0
    flag_label = None
    for prediction in predictions:
        for label in labels:
            if prediction['comment_start_row'] == label['comment_start_row'] and prediction['comment_start_char'] == label['comment_start_char'] and prediction['comment_end_row'] == label['comment_end_row'] and prediction['comment_end_char'] == label['comment_end_char'] and prediction['code_start_row'] == label['code_start_row'] and prediction['code_end_char'] == label['code_end_char'] and prediction['code_end_row'] == label['code_end_row'] and prediction['code_end_char'] == label['code_end_char']:
                # print("exact comment match found")
                flag_label = label
                break
            elif prediction['comment_start_row'] == label['comment_start_row'] and prediction['comment_start_char'] == label['comment_start_char']:
                # print("comment match found")
                flag_label = label
                break
        
        row_number = prediction['code_start_row']
        prediction_length = 0
        label_length = 0
        overlap_length = 0
        while row_number <= prediction['code_end_row']:
            row_pred = code_lines[row_number - 1].rstrip()
            if row_number == prediction['code_end_row']:
                row_pred = row_pred[:(prediction['code_end_char']-1)]
            if row_number == prediction['code_start_row']:
                row_pred = row_pred[(prediction['code_start_char']-1):]
            # print(row_pred)
            prediction_length += len(row_pred)
            if flag_label is not None and row_number >= flag_label['code_start_row'] and row_number <= flag_label['code_end_row']:
                row_label = code_lines[row_number - 1].rstrip()
                if row_number == label['code_end_row']:
                    row_label = row_label[:(label['code_end_char']-1)]
                if row_number == label['code_start_row']:
                    row_label = row_label[(label['code_start_char']-1):]
                label_length += len(row_label)
                # print(row_label)
                overlap_length += min(len(row_pred), len(row_label))

            row_number += 1
        # print("Prediction Length: {}".format(prediction_length))
        # print("Label Length: {}".format(label_length))
        # print("Overlap Length: {}".format(overlap_length))
        if label_length == 0:
            print("Empty Label")
            continue
        accuracy = float(overlap_length)/float(label_length)
        total_accuracy += accuracy
        if verbose == True:
            print("Accuracy: {0:.4f}".format(accuracy))
        comment_count += 1
    if comment_count == 0:
        return 1.0
    average_accuracy = total_accuracy/comment_count
    print('---------------------------------------------------------')
    print('File Name: {}'.format(code_file_name))
    print("Average Accuracy: {}".format(average_accuracy))
    return average_accuracy

def evaluate_set(file_set, verbose):
    dir_name = os.path.dirname('../Dataset')
    if file_set == 'toy_set':
        dir_name += '/Dataset/ToyData/'
    elif file_set == 'real_set':
        dir_name += '/Dataset/RealData/'
    
    accuracy = 0.0
    count = 0
    prediction_files = os.listdir(dir_name + 'Predictions/')
    for filename in prediction_files:
        if filename.endswith('.txt') == True:
            code_file_name = dir_name + 'CodeFiles/' + filename.replace('.txt', '.java')
            label_file_name = dir_name + 'Annotations/' + filename
            prediction_file_name = dir_name + 'Predictions/' + filename
            accuracy += evaluate_single_file(code_file_name, label_file_name, prediction_file_name, False)
            count += 1
            # if verbose == True:
                # print('Accuracy for file ' + filename + ': ' + str(accuracy/count))
    print('Average Accuracy on the entire dataset: {}'.format(accuracy/count))



def main():
    argparser = argparse.ArgumentParser()
    argparser.add_argument('--method', help='evaluate single file or toy dataset or real dataset',
                           choices=['single', 'toy_set', 'real_set'], default='single')
    argparser.add_argument('--verbose', help='enable verbose evaluation report', action='store_true', default=False)

    argparser.add_argument('--code', type=str, default='../Dataset/ToyData/CodeFiles/BubbleSort.java', help='manual annotations file')                       
    argparser.add_argument('--label', type=str, default='../Dataset/ToyData/Annotations/BubbleSort.txt', help='manual annotations file')
    argparser.add_argument('--prediction', type=str, default='./prediction_test1.txt', help='predicted annotations file')
    
    args = argparser.parse_args()

    # print(args.method)
    # print(args.label)
    # print(args.prediction)

    if args.method == 'single':
        evaluate_single_file(args.code, args.label, args.prediction, args.verbose)
    elif args.method == 'toy_set':
        evaluate_set(args.method, args.verbose)
    

if __name__ == "__main__":
    main()  